---
title: 'DATA 622 - Homework #1'
author: "Zach Alexander"
date: "2/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

```{r libraries, warning=FALSE, message=FALSE, echo=FALSE}
require(palmerpenguins)
require(dplyr)
require(ggplot2)
require(tibble)
require(tidyr)
require(tidyverse)
require(dplyr)
require(ggplot2)
require(ggfortify)
require(gridExtra)
require(GGally)
require(corrplot)
require(devtools)
require(caret)
require(regclass)
require(MASS)
require(pROC)
require(nnet)
require(modelr)
require(broom)
require(mnlogit)
require(mlogit)
```

***

#### Part 1: Logistic Regression with binary outcome

**A) The penguin dataset has a 'species' column. Please check how many categories you have in the species column. Conduct whatever data manipulation you need to do to be able to build a logistic regression with binary outcome. Please explain your reasoning behind your decision as you manipulate the outcome/dependent variable (species).**

***

First, we'll save the penguin dataset as a tibble so we can manipulate it easier in the next steps:

```{r, echo=FALSE}
penguins <- tibble(penguins)
```

Then, as mentioned above, we'll first have to identify the number of categories that make up the `species` column. We can do this by running the syntax below:

```{r}
summary(penguins$species)
```

As we can see above, it looks like there are three species present in this dataset. Therefore, in order to create a logistic regression with a binary outcome, we'll have to determine which two species should be combined. We can also see from this summary that there are a small number of Chinstrap penguins in this dataset. This may be a good initial indication that combining this species with either Adelie or Gentoo will be best. However, before doing so, we can also take a look at the summary of all of the variables in the dataset, as well as some of the interactions between our continuous variables, including `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`.  

```{r, echo=FALSE}
summary(penguins)
```

It looks like there are 11 individuals that have missing `sex` information, including two individuals in our dataset that have a large amount of NAs across most of the features. Therefore, I'll omit those from our analysis:

```{r}
penguins <- na.omit(penguins)
```

Next, we can create plots that show interactions between some of our continous variables:
```{r, fig.height=16, fig.width=10, echo=FALSE, warning=FALSE}
plot1 <- ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, col = species)) +
    geom_point() + 
    theme(legend.position="top") +
    labs(title = "Bill Length and Bill Depth")

plot2 <- ggplot(penguins, aes(x = bill_length_mm, y = flipper_length_mm, col = species)) +
    geom_point() + 
    theme(legend.position="top") +
    labs(title = "Bill Length and Flipper Length")

plot3 <- ggplot(penguins, aes(x = bill_length_mm, y = body_mass_g, col = species)) +
    geom_point() + 
    theme(legend.position="top") +
    labs(title = "Bill Length and Body Mass")

plot4 <- ggplot(penguins, aes(x = bill_depth_mm, y = flipper_length_mm, col = species)) +
    geom_point() + 
    theme(legend.position="top") +
    labs(title = "Bill Depth and Flipper Length")

plot5 <- ggplot(penguins, aes(x = bill_depth_mm, y = body_mass_g, col = species)) +
    geom_point() + 
    theme(legend.position="top") + 
    labs(title = "Bill Depth and Body Mass")

plot6 <- ggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm, col = species)) +
    geom_point() + 
    theme(legend.position="top") + 
    labs(title = "Flipper Length and Body Mass")

grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, ncol=2, nrow=3)
```
Interestingly, when we break out the plots by species, we can see that many of the feature interactions show clustering between Adelie and Chinstrap penguins (red and green), while Gentoo penguins tend to contrast the other two species for most interactions. This is also confirmed by most of the single-variable distributions split out by species in the plots below. With the exception of the distribution of `bill_length_mm`, distributions for `body_mass_g`, `bill_depth_mm`, and `flipper_length_mm` all show there to be overlapping distributions between Adelie and Chinstrap penguin species.

```{r, message=FALSE, warning=FALSE, fig.width=12, fig.height=10, echo=FALSE}
penguins %>%
  dplyr::select(species, body_mass_g, bill_length_mm, bill_depth_mm, flipper_length_mm) %>%
  ggpairs(aes(color = species))
```
With this in mind, I think it'll be best to *combine the Chinstrap penguin species with the Adelie species* in order for us to create our binary outcome for our logistic regression. Therefore, to do this, we can do the following data manipulation by creating an extra column to bifurcate the species variable:

```{r}
penguins <- penguins %>% 
  mutate(species_rollup = ifelse(species == 'Adelie' | species == 'Chinstrap', 'Adelie-Chinstrap', 'Gentoo'))
```

And now we can see the split between the two new groups of penguins:
```{r, echo=FALSE}
table(penguins$species_rollup)
```

***

**B) Please make sure you are evaluating the independent variables appropriately in deciding which ones should be in the model.**

***

***Checking kurtosis and transforming variables***  

Now that the outcome variable of species has been rolled up into two distinct groups of penguins, the next step is to evaluate the independent variables in the dataset accordingly. A good first step is to take a look at the distributions of each feature to get a sense of kurtosis:

```{r, warning=FALSE, echo=FALSE}
plot7 <- ggplot(penguins, aes(x=body_mass_g)) + 
    geom_histogram(aes(y=..density..),      
                   binwidth=100,
                   colour="gray", fill="white") +
    geom_density(alpha=.7, fill="#619CFF") +
   labs(title = "Body Mass Distribution")

plot8 <- ggplot(penguins, aes(x=bill_length_mm)) + 
    geom_histogram(aes(y=..density..),      
                   binwidth=1,
                   colour="gray", fill="white") +
    geom_density(alpha=.7, fill="#619CFF") +
    labs(title = "Bill Length (mm) Distribution")

plot9 <- ggplot(penguins, aes(x=bill_depth_mm)) + 
    geom_histogram(aes(y=..density..),    
                   binwidth=0.5,
                   colour="gray", fill="white") +
    geom_density(alpha=.7, fill="#619CFF")  +
    labs(title = "Bill Depth (mm) Distribution")

plot10 <- ggplot(penguins, aes(x=flipper_length_mm)) + 
    geom_histogram(aes(y=..density..),    
                   binwidth=2.5,
                   colour="gray", fill="white") +
    geom_density(alpha=.7, fill="#619CFF")  +
    labs(title = "Flipper Length (mm) Distribution")

grid.arrange(plot7, plot8, plot9, plot10, ncol=2, nrow=2)
```

From the above distributions, we can see that most of our continuous variables are quite skewed. In order to alleviate this before subjecting to our logistic regression, we can perform some transformations to make more normal distributions.

```{r}
# transform variables
penguins$bill_length_transformed <- abs(penguins$bill_length_mm - mean(penguins$bill_length_mm))^(1/2)
penguins$flipper_length_transformed <- abs(penguins$flipper_length_mm - mean(penguins$flipper_length_mm))^(1/2)
```

```{r, echo=FALSE}
plot11 <- ggplot(penguins, aes(x=bill_length_transformed)) + 
    geom_histogram(aes(y=..density..),      
                   binwidth=0.15,
                   colour="gray", fill="white") +
    geom_density(alpha=.7, fill="#F8776D") +
    labs(title = "Bill Length -- Transformed")

plot12 <- ggplot(penguins, aes(x=flipper_length_transformed)) + 
    geom_histogram(aes(y=..density..),      
                   binwidth=0.15,
                   colour="gray", fill="white") +
    geom_density(alpha=.7, fill="#F8776D") +
    labs(title = "Flipper Length -- Transformed")


grid.arrange(plot8, plot11, plot10, plot12, ncol=2, nrow=2)
```

As we can see above, we were able to perform a few transformations on `bill_length_mm` and `flipper_length_mm` to make more normal distributions. However, we will have to see if these transformations are interpretable when we run our logistic regression.  

***

***Checking for multicollinearity***  

Additionally, when evaluating the independent variables, it's important to take multicollinearity into consideration. Multicollinearity can impact the variances of our parameter estimates, which could lead to incorrect inferences about relationships between our dependent variable (species) and independent variables. Therefore, I ran a correlation plot to check the collinearity between some of our independent variables.

```{r, echo=FALSE}

penguins_cont <- penguins %>% dplyr::select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)


correlation = cor(penguins_cont, use = 'pairwise.complete.obs')

corrplot(correlation, method='ellipse')
```

We can see above that `flipper_length` and `body_mass_g` have a pretty strong positive correlation with one another. This makes sense given that typically penguins with larger body mass will have larger flippers. Since we are seeing a bimodal distribution for `flipper_length` which led to a pretty in-depth transformation, we may consider excluding this feature from our logistic regression, given that it also shows such a strong relationship with `body_mass_g`.

***

***Downsampling Adelie-Chinstrap from dataset***  

One final step is to take a quick look at the counts for our binary variable `species_rollup` that we created earlier to help us create our binary logistic regression outcome:

```{r}
table(penguins$species_rollup)
```

Unfortunately, it looks like there's a class imbalance here where there are almost double the amount of Adelie-Chinstrap individuals in our dataset than Gentoo individuals. This could impact our logistic regression outcome and eventual model performance. In order to create a more equal subset of individuals between the two classes, we can downsample the Adelie-Chinstrap group to match the number of Gentoo species at 119. In order to do this, we can run a function in the `caret` package called `downSample()`, which will randomly select 119 individuals from the Adelie-Chinstrap group to use for our model:

```{r}
penguins_rollup_factor <- factor(penguins$species_rollup)
downsample_penguins <- downSample(penguins, penguins_rollup_factor, list = FALSE, yname = "Class")

table(downsample_penguins$species_rollup)
```

Now, we have an equal share of 119 individuals in our new `downsample_penguins` dataset between our two groups. With these adjustments, we are now ready to start running our logistic regression models.

```{r}
downsample_penguins$penguins_species_fnl <- ifelse(downsample_penguins$species_rollup == 'Gentoo', 0, 1)
```

***

***Running the logistic regression***  

First, we'll need to split our `downsample_penguins` dataset into a training and testing set (80% training, 20% testing). This was necessary in order to measure our model performance on our holdout test set later on.  

```{r}
set.seed(1234567)

# utilizing one dataset for all four models
penguins_partition <- createDataPartition(downsample_penguins$species_rollup, p=0.8, list=FALSE)
penguins_training <- downsample_penguins[penguins_partition,]
penguins_testing <- downsample_penguins[-penguins_partition,]
```

Next, after running a few preliminary models that didn't show large differences between the transformed variables I had created and those from the original dataset, I decided to keep the existing variables in order to ease interpretation of the log odds. Additionally, I decided to not use variables such as `bill_depth_mm`, `island`, `sex`, and `year`, since they appear to be overfitting the model and creating fitted probabilities that numerically turned out to be 0 or 1. Therefore, for the first model, I added three variables `bill_length_mm`, `flipper_length_mm`, and `body_mass_g` to the logistic regression:
```{r, warning=FALSE}
model1 <- glm(penguins_species_fnl ~ bill_length_mm +  flipper_length_mm + body_mass_g , data = penguins_training, family = binomial)
summary(model1)
```
As we can see from the model summary, p-values for `flipper_length_mm` and `body_mass_g` fell below 0.05, where `bill_length_mm` was above this threshold. In my next model, I decided to run the `stepAIC()` function in order to conduct stepwise selection and identify the explanatory variables that were most statistically significant. However, before doing so, I did want to check my VIF scores in order to check multicollinearity. 
```{r}
# print variable inflation factor score
print('VIF scores of predictors')

VIF(model1)
```

Fortunately, we can see that all VIF scores are quite low (below 5), meaning that interactions with one another are quite low. In Model 2, I used stepwise selection to pare down the explanatory variables.

```{r, warning=FALSE}
model2 <- model1 %>% stepAIC(trace=FALSE)
summary(model2)
```
As we can see, it doesn't look like StepAIC removed any of our less statistically significant variables, since they didn't meet the threshold. However, in model 3, I decided to remove `bill_length_mm` to see if this has an impact on our AIC score.

```{r, warning=FALSE}
model3 <- glm(penguins_species_fnl ~ flipper_length_mm + body_mass_g, data = penguins_training, family = binomial)
summary(model3)
```
After running this third model, we can see a small increase in the AIC. I also noticed through this process that `flipper_length_mm` seemed to be quite predictive of our bifurcated species variable. Therefore, I ran one final model which just exposed `flipper_length_mm` as the explanatory variable:

```{r, warning=FALSE}
model4 <- glm(penguins_species_fnl ~ flipper_length_mm, data = penguins_training, family = binomial)
summary(model4)
```
This model showed a slight increase in AIC from our previous models, and I do worry that if this model were to be used it may be overfitted to our training dataset. We can examine these models in the next part during the evaluation stage. However, I'll quickly breakdown some of the log odds information and interpret the variables of these logistic regression models below.

***

***Interpreting the explanatory variables***  

If we take a look at the initial model created (model #1) and examine the summary output, we can find the following:  

+ *Bill Length (mm)* -- as the bill length of an individual penguin in the dataset increases by one millimeter, then the odds of the individual being of the Adelie or Chinstrap species increases by exp(0.47), or 1.6.  

+ *Flipper Length (mm)* -- as the flipper length of an individual penguin in the dataset increases by one millimeter, then the odds of the individual being of the Adelie or Chinstrap species decreases by exp(-0.81), or 0.44.  

+ *Body Mass (g)* -- as the body mass of an individual penguin in the dataset increases by one gram, then the odds of the individual being of the Adelie or Chinstrap species decreases by exp(-0.003), or 0.10.


***

#### Part 2: Logistic Regression evaluation (AUC, TPR, FPR, TNR, FNR and Accuracy)  

**A) For your model from #1, please provide: AUC, Accuracy, TPR, FPR, TNR, FNR**

***

Although we won't evaluate the performance of this model on the training dataset, it is good to check to make sure these functions are working properly before subjecting this to the testing set. Therefore, below we will examine the predictions and confusion matrix for our model of choice (model #1):
```{r}
predictTrain = predict(model1, type = "response")
penguins_training$target = round(predictTrain, 1)
table(penguins_training$penguins_species_fnl, predictTrain > 0.5)
```

We can also check the accuracy of the predictions on the training set:
```{r}
penguins_training = penguins_training %>%
  mutate(accuracy = 1*(round(target, 0) == round(penguins_species_fnl, 0)))

paste0('Accuracy on Training Set: ', sum(penguins_training$accuracy)/nrow(penguins_training))
```

***

***Measuring Accuracy, TPR, FPR, TNR, and FNR on testing set***

Now, we can evaluate our chosen model #1 on our holdout testing set. Initially, we'll set our threshold at 0.5, to determine whether or not our predictions should be classified into the Gentoo category (less than or equal to 0.5) or Adelie-Chinstrap category (greater than 0.5):

```{r}
predictTest = predict(model1, type = "response", newdata = penguins_testing)
penguins_testing$target = round(predictTest, 1)
confusion_matrix <- table(penguins_testing$penguins_species_fnl, predictTest > 0.5)
confusion_matrix
```

***Calculating TPR, TNR, FPR, and FNR***  

```{r}
# tp = true positive
# tn = true negative
# fp = false positive
# fn = false negative

tp <- confusion_matrix[4]
tn <- confusion_matrix[1]
fp <- confusion_matrix[2]
fn <- confusion_matrix[3]
p <- sum(confusion_matrix[3], confusion_matrix[4])
n <- sum(confusion_matrix[1], confusion_matrix[2])

tpr <- tp / p
tnr <- tn / n
fpr <- fp / n
fnr <- fn / p

title_row <- c('True Positive Rate (TPR)', 'True Negative Rate (TNR)', 'False Positive Rate (FPR)', 'False Negative Rate (FNR)')
values <- c(tpr, tnr, fpr, fnr)

results <- data.frame(title_row, values)
colnames(results) <- c('Measure', 'Value')

results
```
After calculating TPR, TNR, FPR, and FNR, it's a good sign to see that both FPR and FNR are quite low. Additionally, the model was very good at classifying penguins in the testing dataset that were of either the Adelie/Chinstrap species (predicted all of them correctly), and was close to classifying all Gentoo species correctly, with only one Gentoo penguin mistakenly being classified as an Adelie/Chinstrap species.


***Accuracy***

```{r}
penguins_testing = penguins_testing %>%
  mutate(accuracy = 1*(round(target, 0) == round(penguins_species_fnl, 0)))

paste0('Accuracy on Training Set: ', sum(penguins_testing$accuracy)/nrow(penguins_testing))
```
As we can see above, when checking the accuracy of the predictions on our testing set, we can see that model 1 was roughly 98% accurate in predicting whether or not the penguin was classified as either a Gentoo or Adelie/Chinstrap species.

***Calculating AUC***

We can plot an ROC curve and calculate our AUC by using the `pRoc` package. Below, we were able to plot our thresholds.

```{r, warning=FALSE, message=FALSE, cache=TRUE}
roc(penguins_testing$penguins_species_fnl ~ predictTest, plot=TRUE, print.auc=TRUE, col='black', lwd=4, legacy.axes=TRUE)
```

Finally, the AUC of our chosen model #1 is very good, with a value of 0.991, which is very close to 1 and supports our calculation of accuracy that almost all predictions were correct.  


***

#### Part 3: Multinomial Logistic Regression  

**A) Please fit a multinomial logistic regression where your outcome variable is 'species'.**  
**B) Please be sure to evaluate the independent variables appropriately to fit your best parsimonious model.**  

For this last part, we'll use the initial `species` variable to create a multinomial logistic regression model. This way, we do not have to bifurcate our species into two categories, but instead use a different type of logistic regression to obtain predictions for all three species. In order to do this, I decided to take our initial training dataset from Part 1, since it is already split accordingly, and use the `multinom()` function from the `broom` package to run our first model. 

```{r, echo=FALSE}
penguins_training_mod <- penguins_testing %>% dplyr::select(-target, -accuracy)
penguins_training_mod$outcome_level = relevel(penguins_training_mod$species, ref = "Gentoo")
```

In this initial model, I kept all variables in, just to get a baseline AIC to compare future models on the training data. 
```{r}
multinom_model1 <- multinom(outcome_level ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g + sex + year + bill_length_transformed + flipper_length_transformed, data = penguins_training_mod)
tidy(multinom_model1, conf.int = FALSE, conf.level = 0.95, exponentiate = FALSE)
summary(multinom_model1)
```

We can see that the AIC value is around 36, and when we look at the p-values and summary statistics, we can immediately see that it'll be best to remove variables such as `year` and `sex`, both of which didn't perform well in our binomial logistic regression earlier, as well as `bill_length_transformed` and `flipper_length_transformed`, both of which have high p-values here as well.  

When we re-run the model, removing these variables, we can see that there is an improvement in AIC (from 36 to 20). However, we'll continue to remove a few variables to see if we can improve our model even more.
```{r}
multinom_model2 <- multinom(outcome_level ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g, data = penguins_training_mod)
tidy(multinom_model2, conf.int = FALSE, conf.level = 0.95, exponentiate = FALSE)
summary(multinom_model2)
```


In our third iteration of our model, we decided to remove `flipper_length_mm`, which seemed to have the highest p-value and was likely exposing our model to multicollinearity issues with `body_mass_g`, which we could see from our initial variable examination earlier in our model building.  


```{r}
multinom_model3 <- multinom(outcome_level ~ bill_length_mm + bill_depth_mm  + body_mass_g, data = penguins_training_mod)
tidy(multinom_model3, conf.int = FALSE, conf.level = 0.95, exponentiate = FALSE)
summary(multinom_model3)
```

We can see that by removing `flipper_length_mm`, AIC continues to improve (dropping from 20 to 16). In our final iteration, we'll remove `bill_depth_mm` given additional multicollinearity issues `body_mass_g`.

```{r}
multinom_model4 <- multinom(outcome_level ~ bill_length_mm  + body_mass_g, data = penguins_training_mod)
tidy(multinom_model4, conf.int = FALSE, conf.level = 0.95, exponentiate = FALSE)
summary(multinom_model4)
```

This final model yielded our lowest AIC at 12.8, and contains `bill_length_mm` and `body_mass_g` as our chosen independent variables for our best parsimonious model.

***